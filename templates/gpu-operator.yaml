# The NVIDIA GPU Operator uses the operator framework within Kubernetes to automate the 
# management of all NVIDIA software components needed to provision GPU. 
# These components include the NVIDIA drivers (to enable CUDA), Kubernetes 
# device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling, 
# DCGM based monitoring and others.
# https://github.com/NVIDIA/gpu-operator
#
# Prevent from running on a specific node
#
# kubectl label --overwrite \
#        node ${NODE_NAME} \
#        nvidia.com/gpu.deploy.operator-validator=false \
#        nvidia.com/gpu.deploy.driver=false \
#        nvidia.com/gpu.deploy.container-toolkit=false \
#        nvidia.com/gpu.deploy.device-plugin=false \
#        nvidia.com/gpu.deploy.gpu-feature-discovery=false \
#        nvidia.com/gpu.deploy.dcgm-exporter=false \
#        nvidia.com/gpu.deploy.dcgm=false

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: nvidia-gpu-operator
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  project: default
  source:
    chart: gpu-operator
    repoURL: https://helm.ngc.nvidia.com/nvidia
    targetRevision: 22.9.2
 #   helm:
 #     skipCrds: false
 #     values: |
 #       cdi: 
 #         enabled: "true"
 #         default: "false"
 #       driver:
 #         enabled: "true"
 #       toolkit:
 #         enabled: "true"
  destination:
    server: "https://kubernetes.default.svc"
    namespace: gpu-operator
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: gpu-exporter
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  source:
    repoURL: 'https://utkuozdemir.org/helm-charts'
    targetRevision: 0.3.1
    chart: nvidia-gpu-exporter
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: gpu-operator
  project: default
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
